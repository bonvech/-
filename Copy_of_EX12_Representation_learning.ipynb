{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bonvech/-/blob/master/Copy_of_EX12_Representation_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G7FVEyM2ATk"
      },
      "source": [
        "# Функции для обучения автоэнкодеров"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVsRWof9kTWs",
        "outputId": "70f681e7-2e2b-40a6-d508-cbdfa13ba626"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.3/802.3 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eZMTlqGx2ATn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import lightning as L\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "from warnings import simplefilter\n",
        "\n",
        "simplefilter(\"ignore\", UserWarning)\n",
        "simplefilter(\"ignore\", RuntimeWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fHWUBr52ATo"
      },
      "outputs": [],
      "source": [
        "def plot_manifold(latent_r, labels=None, alpha=0.9, classes=None, title=None):\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    if labels is None:\n",
        "        plt.scatter(latent_r[:, 0], latent_r[:, 1], alpha=alpha)\n",
        "    else:\n",
        "        plt.scatter(latent_r[:, 0], latent_r[:, 1], c=labels, cmap=\"tab10\", alpha=alpha)\n",
        "        cbar = plt.colorbar()\n",
        "    if classes:\n",
        "        num_of_classes = list(range(0, len(classes)))\n",
        "        cbar.ax.set_yticks(num_of_classes)\n",
        "        cbar.ax.set_yticklabels(classes)\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# plotting reconstructed and noised images\n",
        "def plot_samples(*args, digit_size=28, name=None, single_size=2):\n",
        "    args = [x.squeeze() for x in args]\n",
        "    n = min([x.shape[0] for x in args])\n",
        "    figure = np.zeros((digit_size * len(args), digit_size * n))\n",
        "\n",
        "    for i in range(n):\n",
        "        for j in range(len(args)):\n",
        "            figure[\n",
        "                j * digit_size : (j + 1) * digit_size,\n",
        "                i * digit_size : (i + 1) * digit_size,\n",
        "            ] = args[j][i].squeeze()\n",
        "\n",
        "    plt.figure(figsize=(single_size * n, single_size * len(args)))\n",
        "\n",
        "    plt.imshow(figure, cmap=\"gray_r\", clim=(0, 1))\n",
        "\n",
        "    plt.grid(False)\n",
        "    ax = plt.gca()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    if name is not None:\n",
        "        plt.savefig(name)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKITefMs2ATp"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        hidden_dims = [32, 64, 128, 256]  # num of filters in layers\n",
        "        modules = []\n",
        "        in_channels = 1  # initial value of channels\n",
        "        for h_dim in hidden_dims:  # conv layers\n",
        "            modules.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(\n",
        "                        in_channels=in_channels,  # num of input channels\n",
        "                        out_channels=h_dim,  # num of output channels\n",
        "                        kernel_size=3,\n",
        "                        stride=2,  # convolution kernel step\n",
        "                        padding=1,  # save shape\n",
        "                    ),\n",
        "                    nn.BatchNorm2d(h_dim),\n",
        "                    nn.LeakyReLU(),\n",
        "                )\n",
        "            )\n",
        "            in_channels = h_dim  # changing number of input channels for next iteration\n",
        "\n",
        "        modules.append(nn.Flatten())  # to vector, size 256 * 2 * 2 = 1024\n",
        "        modules.append(nn.Linear(256 * 2 * 2, latent_dim))\n",
        "\n",
        "        self.encoder = nn.Sequential(*modules)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        hidden_dims = [256, 128, 64, 32]  # num of filters in layers\n",
        "        self.linear = nn.Linear(in_features=latent_dim, out_features=1024)\n",
        "\n",
        "        modules = []\n",
        "        for i in range(len(hidden_dims) - 1):  # define upsample layers\n",
        "            modules.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Upsample(scale_factor=2),\n",
        "                    nn.Conv2d(\n",
        "                        in_channels=hidden_dims[i],\n",
        "                        out_channels=hidden_dims[i + 1],\n",
        "                        kernel_size=3,\n",
        "                        padding=1,\n",
        "                    ),\n",
        "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
        "                    nn.LeakyReLU(),\n",
        "                )\n",
        "            )\n",
        "\n",
        "        modules.append(\n",
        "            nn.Sequential(\n",
        "                nn.Upsample(scale_factor=2),\n",
        "                nn.Conv2d(in_channels=hidden_dims[-1], out_channels=1, kernel_size=5),\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(*modules)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)  # from latents space to Linear\n",
        "        x = x.view(-1, 256, 2, 2)  # reshape\n",
        "        x = self.decoder(x)  # reconstruction\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3wsP06D2ATp"
      },
      "outputs": [],
      "source": [
        "class LitAE(L.LightningModule):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=1e-4)\n",
        "\n",
        "    def forward_handler(self, data, *args, **kwargs):\n",
        "        # here is the logic how data is moved through AE\n",
        "        latent = self.encoder(data)\n",
        "        recon = self.decoder(latent)\n",
        "        return latent, recon\n",
        "\n",
        "    def loss_handler(self, recon, data, *args, **kwargs):\n",
        "        # here is the loss function computing\n",
        "        loss = F.mse_loss(F.sigmoid(recon), data)\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        data, labels = batch\n",
        "\n",
        "        latent, recon = self.forward_handler(data, labels)\n",
        "        loss = self.loss_handler(recon, data, latent)\n",
        "\n",
        "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        data, labels = batch\n",
        "\n",
        "        latent, recon = self.forward_handler(data, labels)\n",
        "        loss = self.loss_handler(recon, data, latent)\n",
        "\n",
        "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def on_test_epoch_start(self):\n",
        "        # create dict with empty tensors for further accumulating over batches\n",
        "        self.test_result = defaultdict(torch.Tensor)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        data, labels = batch\n",
        "\n",
        "        latent, recon = self.forward_handler(data, labels)\n",
        "        recon = F.sigmoid(recon)\n",
        "        self.update_test_result(data, recon, latent, labels)\n",
        "\n",
        "    def update_test_result(self, data, recon, latent, labels):\n",
        "        # accumulating results every batch\n",
        "        self.test_result[\"real\"] = torch.cat([self.test_result[\"real\"], data.cpu()])\n",
        "        self.test_result[\"recon\"] = torch.cat([self.test_result[\"recon\"], recon.cpu()])\n",
        "        self.test_result[\"latent\"] = torch.cat(\n",
        "            [self.test_result[\"latent\"], latent.cpu()]\n",
        "        )\n",
        "        self.test_result[\"labels\"] = torch.cat(\n",
        "            [self.test_result[\"labels\"], labels.cpu()]\n",
        "        )\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "        # simply change type from torch tensor to numpy array\n",
        "        # for every item in test_result dictionary\n",
        "        for key in self.test_result:\n",
        "            self.test_result[key] = self.test_result[key].numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uhyDZYe2ATq"
      },
      "source": [
        "# Задание 1. Автоэнкодер для FashionMNIST c SSIM loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw-wocjn2ATq"
      },
      "source": [
        "В этом задании требуется обучить автоэнкодер на датасете FashionMNIST.\n",
        "\n",
        "* Используйте энкодер и декодер из лекции (код `Encoder`, `Decoder` выше).\n",
        "* Замените функцию потерь при обучении автоэнкодера на `SSIM loss` [📚[wiki]](https://ru.wikipedia.org/wiki/SSIM).\n",
        "* Обучите автоэнкодер с размером латентного слоя 2.\n",
        "* Выведите латентное представление объектов на плоскости. Проанализируйте, разделяются ли в нем классы?\n",
        "\n",
        "Далее:\n",
        "\n",
        "* Обучите автоэнкодер с размером латентного слоя 30.\n",
        "* Продемонстрируйте восстановление автоэнкодером переданных ему изображений.\n",
        "\n",
        "Напишите выводы."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRXR7uuF2ATq"
      },
      "source": [
        "**Подсказки:**\n",
        "\n",
        "* Для обучения и тестирования воспользуйтесь Lightning-модулем `LitAE` (код выше). Для того, чтобы заменить функцию потерь, рекомендуется отнаследоваться от `LitAE` и переопределить метод `loss_handler`, заменив функцию потерь в нем.\n",
        "* Для визуализации латентных представлений на плоскости рекомендуется использовать функцию `plot_manifold` (код выше).\n",
        "* Для визуализации оригинальных изображений и их реконструкций рекомендуется использовать функцию `plot_samples` (код выше)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyemPoff2ATr"
      },
      "source": [
        "Импорт необходимых библиотек:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6Xpz9X62ATr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import lightning as L\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from lightning.pytorch.loggers import TensorBoardLogger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6GIalte2ATr"
      },
      "source": [
        "Загрузка датасета:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eenu0a6B2ATs"
      },
      "outputs": [],
      "source": [
        "root = \"./data\"\n",
        "\n",
        "train_set = FashionMNIST(\n",
        "    root=root, train=True, transform=transforms.ToTensor(), download=True\n",
        ")\n",
        "test_set = FashionMNIST(\n",
        "    root=root, train=False, transform=transforms.ToTensor(), download=True\n",
        ")\n",
        "\n",
        "torch.manual_seed(42)\n",
        "train_set, val_set = random_split(train_set, lengths=[50000, 10000])\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_set, batch_size=batch_size, shuffle=True, num_workers=2\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_set, batch_size=batch_size, shuffle=False, num_workers=2\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_set, batch_size=batch_size, shuffle=False, num_workers=2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2O4RUxs2ATs"
      },
      "source": [
        "В датасете 10 классов, каждый соответствует одному из предметов гардероба."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEo0Yy1-2ATs"
      },
      "outputs": [],
      "source": [
        "# Method for display all class samples from dataset\n",
        "\n",
        "\n",
        "def show_dataset(dataset):\n",
        "    fig, axs = plt.subplots(1, len(dataset.classes), figsize=(20, 5))\n",
        "    for cls_num, name in enumerate(dataset.classes):\n",
        "        i = np.argwhere(dataset.targets == cls_num)[0][0]\n",
        "        ax = axs[cls_num]\n",
        "        ax.imshow(dataset.data[i], cmap=\"gray_r\")\n",
        "        ax.set_title(name)\n",
        "        ax.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZfqFL5C2ATt"
      },
      "outputs": [],
      "source": [
        "show_dataset(test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y6SdI7U2ATt"
      },
      "source": [
        "Задайте `latent_dim` равным 2 для последующего построения отображения латентных представлений на плоскости. Инициализируйте модели энкодера и декодера."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKmwfX5W2ATt"
      },
      "outputs": [],
      "source": [
        "L.seed_everything(42)\n",
        "\n",
        "latent_dim =   # Your code here\n",
        "encoder =  # Your code here\n",
        "decoder =  # Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OOzyEcB2ATt"
      },
      "source": [
        "Опишите класс-наследник `LitAE`, который будет использовать `SSIM loss` [📚[wiki]](https://ru.wikipedia.org/wiki/SSIM) в качестве функции потерь для автоэнкодера внутри `loss_handler`. Можно воспользоваться готовой [реализацией 🐾[git]](https://github.com/VainF/pytorch-msssim):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f16WfgwK2ATt"
      },
      "outputs": [],
      "source": [
        "class LitAE_with_SSIM(LitAE):\n",
        "    def loss_handler(self, recon, data, *args, **kwargs):\n",
        "        # Your code here\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FrXd4-L2ATt"
      },
      "source": [
        "Наконец, обучите автоэнкодер."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfEXMjfH2ATu"
      },
      "outputs": [],
      "source": [
        "from lightning.pytorch.loggers import TensorBoardLogger\n",
        "\n",
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvkLbLX22ATu"
      },
      "source": [
        "Для прогона тестовых данных через автоэнкодер воспользуйтесь `trainer.test`. Результат тестирования будет записан в словарь в атрибуте `.test_result` Lightning-модели автоэнкодера."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCc8A7Ws2ATu"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbvnZjjo2ATu"
      },
      "source": [
        "Выведите латентное представление объектов на плоскости. Проанализируйте, разделяются ли в нем классы? Какие классы смешиваются, а какие хорошо отделяются?\n",
        "\n",
        "У функции `plot_manifold` есть аргумент `classes`. В него можно передать список названий классов (`test_set.classes`) для отображения названий вместо номеров классов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfWEGwS82ATu"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsFywqs12ATu"
      },
      "source": [
        "*Ваши наблюдения:*\n",
        "\n",
        "***Your text here***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnMFWwqk2ATu"
      },
      "source": [
        "Теперь обучите автоэнкодер с размером латентного слоя 30."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Swhcw922ATv"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xtu0MEDq2ATv"
      },
      "source": [
        "Обработайте автоэнкодером тестовые данные. Продемонстрируйте восстановление автоэнкодером переданных ему изображений. Используйте функцию `plot_samples`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMnPgOco2ATv"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKw7mqIo2ATv"
      },
      "source": [
        "Какие особенности в восстановленных изображениях по сравнению с исходными вы можете отметить?\n",
        "\n",
        "*Ваши наблюдения:*\n",
        "\n",
        "***Your text here***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGE0_krH2ATv"
      },
      "source": [
        "## Формат результата"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqmANHll2ATv"
      },
      "source": [
        "1. Визуализация латентного пространства для автоэнкодера с `latent_dim=2`, **выводы**. Пример визуализации:\n",
        "\n",
        "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/Exercises/EX12/result_1_1_task_ex12.png\" width=\"500\"/>\n",
        "\n",
        "\n",
        "2. Изображения, восстановленные автоэнкодером c `latent_dim=30`, **выводы**. Пример изображений:\n",
        "\n",
        "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/Exercises/EX12/result_2_1_task_ex12.png\" width=\"500\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fYKVaeA2ATv"
      },
      "source": [
        "# Задание 2. Обнаружение аномалий с помощью автоэнкодера"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FT8yP5j2ATw"
      },
      "source": [
        "Представим следующую ситуацию: нам нужна система, которая принимает изображение сетчатки глаза. Если изображение не является фотографией сетчатки глаза, то система должна сообщить пользователю об ошибке и не принимать это изображение.\n",
        "\n",
        "Идея заключается в следующем: если автоэнкодер может выучить внутреннее представление данных, например, фотографий сетчатки глаза, то при восстановлении данных, которые не являются фотографией сетчатки глаза, ошибка будет существенно больше. Установив порог этой ошибки, мы сможем отделять нужные фотографии от ненужных.\n",
        "\n",
        "* Обучите автоэнкодер на фотографиях сетчатки глаза (RetinaMNIST).\n",
        "* Подайте в автоэнкодер другое изображение (здесь будет использован BloodMNIST).\n",
        "* Посчитайте ошибку восстановления для разных датасетов.\n",
        "* Установите порог (значение ошибки) для определения класса фотографии (сетчатка глаза или нет).\n",
        "* Проведите тесты.\n",
        "* Напишите выводы."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MmqBBTZ2ATw"
      },
      "source": [
        "Установка и импорт необходимых библиотек:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4qN2Kig2ATw"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade git+https://github.com/MedMNIST/MedMNIST.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEUN2S6u2ATw"
      },
      "outputs": [],
      "source": [
        "import medmnist\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightning as L\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from medmnist import INFO\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "from lightning.pytorch.loggers import TensorBoardLogger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGSghZqD2ATw"
      },
      "source": [
        "[[doc] 🛠️ MedMNIST](https://medmnist.com/) — это набор медицинских датасетов, по формату повторяющих MNIST:\n",
        "\n",
        "* размер изображений 28×28,\n",
        "* совместимы с PyTorch ([пример использования🐾[git]](https://github.com/MedMNIST/MedMNIST/blob/main/examples/getting_started.ipynb))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBR9V09X2ATx"
      },
      "source": [
        "В этом задании будем работать с двумя датасетами: RetinaMNIST и BloodMNIST.\n",
        "\n",
        "Первый содержит изображения сетчатки глаз, второй — клеток крови."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed_maIZ-2ATx"
      },
      "outputs": [],
      "source": [
        "import medmnist\n",
        "from medmnist import INFO\n",
        "\n",
        "DataClass = getattr(medmnist, INFO[\"retinamnist\"][\"python_class\"])\n",
        "\n",
        "train_retina_dataset = DataClass(split=\"train\", download=True)\n",
        "val_retina_dataset = DataClass(split=\"val\", download=True)\n",
        "\n",
        "plt.imshow(val_retina_dataset[0][0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj2KkBdS2ATy"
      },
      "source": [
        "Несмотря на заявленную совместимость с PyTorch, у датасетов нет свойств `targets` и `data`.\n",
        "\n",
        "Но есть словарь `info` ([код 🐾[git]](https://github.com/MedMNIST/MedMNIST/blob/main/medmnist/dataset.py))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6aMHDLr2ATy"
      },
      "outputs": [],
      "source": [
        "info = val_retina_dataset.info\n",
        "info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu7ubCaQ2ATy"
      },
      "source": [
        "В нем есть аналогичное `targets` свойство `labels`, а у самого объекта есть массив `imgs`, аналогичный `data` в torchvision датасетах."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huBEEcXw2ATy"
      },
      "outputs": [],
      "source": [
        "print(\"labels\", val_retina_dataset.labels.shape)\n",
        "print(\"images\", val_retina_dataset.imgs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKerROIW2ATy"
      },
      "source": [
        "Используя эти свойства, можно сделать датасет совместимым с нашим кодом, написанным для MNIST:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJvmZ3v52ATy"
      },
      "outputs": [],
      "source": [
        "def cast2torch(ds):\n",
        "    ds.targets = ds.labels\n",
        "    ds.data = ds.imgs\n",
        "    ds.classes = list(info[\"label\"].values())\n",
        "\n",
        "\n",
        "cast2torch(val_retina_dataset)\n",
        "cast2torch(train_retina_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGh102Il2ATz"
      },
      "source": [
        "**RetinaMNIST classes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXs0e1bv2ATz"
      },
      "outputs": [],
      "source": [
        "show_dataset(val_retina_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzCmaUHK2ATz"
      },
      "source": [
        "Названий классов тут нет, но для нашей задачи они нам и не понадобятся.\n",
        "Теперь загрузим датасет с клетками крови:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2w7BdZd72ATz"
      },
      "outputs": [],
      "source": [
        "DataClass = getattr(medmnist, INFO[\"bloodmnist\"][\"python_class\"])\n",
        "test_blood_dataset = DataClass(split=\"test\", download=True)\n",
        "cast2torch(test_blood_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BloodMNIST classes**"
      ],
      "metadata": {
        "id": "EheGYACmcAkr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qN_5ndHF2ATz"
      },
      "outputs": [],
      "source": [
        "show_dataset(test_blood_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmYIrvOc2ATz"
      },
      "source": [
        "По форме клетки крови отдаленно напоминают фото сетчатки.\n",
        "\n",
        "Представим, что при сборе новой порции данных фотографии клеток крови по ошибке попали в папку с фотографиями сетчатки и были включены в новую версию датасета.\n",
        "\n",
        "Нам надо исправить ошибку и отделить фото клеток сетчатки от фото клеток крови.\n",
        "\n",
        "Для этого обучим автоэнкодер на той части датасета, которая была собрана раньше и не содержит ошибок. Будем считать, что это `train_retina_dataset`.\n",
        "\n",
        "Обучим на них автоэнкодер. Для этого создадим загрузчики и добавим трансформации к датасетам. В частности, сделаем изображения черно-белыми, чтобы использовать пайплайн из предыдущего задания."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmIUBkFi2AT0"
      },
      "outputs": [],
      "source": [
        "data_transform = transforms.Compose(\n",
        "    [\n",
        "        # Convert all images to Grayscale for sake of MNIST compatibility\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_retina_dataset.transform = data_transform\n",
        "val_retina_dataset.transform = data_transform\n",
        "test_blood_dataset.transform = data_transform\n",
        "\n",
        "# encapsulate data into dataloader form\n",
        "train_loader = DataLoader(\n",
        "    train_retina_dataset, batch_size=128, shuffle=True, num_workers=2\n",
        ")\n",
        "val_loader = DataLoader(val_retina_dataset, batch_size=1, shuffle=False, num_workers=2)\n",
        "blood_loader = DataLoader(\n",
        "    test_blood_dataset, batch_size=1, shuffle=False, num_workers=2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umEeH5iF2AT0"
      },
      "source": [
        "Создайте энкодер и декодер (`latent_dim=2`) и обучите AE 15–30 эпох (занимает несколько секунд).\n",
        "\n",
        "Можно использовать `LitAE`, в котором в качестве функции потерь используется MSE, или `LitAE_with_SSIM`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ek-JIIS52AT0"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8EhZvLX2AT0"
      },
      "source": [
        "Посмотрим, как автоэнкодер научился восстанавливать изображения сетчатки, на которых его учили:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYRHHGuW2AT0"
      },
      "outputs": [],
      "source": [
        "def get_mse_and_show_results(dataloader):\n",
        "    trainer.test(autoencoder, dataloader)\n",
        "    run_res = autoencoder.test_result\n",
        "\n",
        "    real = run_res[\"real\"]\n",
        "    recon = run_res[\"recon\"]\n",
        "\n",
        "    cnt = len(real)\n",
        "    mse = mean_squared_error(\n",
        "        real.reshape(cnt, -1), recon.reshape(cnt, -1), multioutput=\"raw_values\"\n",
        "    )\n",
        "\n",
        "    plot_samples(real[0:5], recon[0:5])\n",
        "    return mse\n",
        "\n",
        "\n",
        "mse_retina = get_mse_and_show_results(val_loader)\n",
        "print(\"MSE for correct retina images\", mse_retina.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn_UrHko2AT1"
      },
      "source": [
        "Теперь посмотрим, что будет, если попытаться восстановить изображения клеток крови:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aE7vgnCJ2AT1"
      },
      "outputs": [],
      "source": [
        "mse_blood = get_mse_and_show_results(blood_loader)\n",
        "print(\"MSE for blood cell images\", mse_blood.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtQyqG6F2AT1"
      },
      "source": [
        "Видно, что автоэнкодер пытается восстанавливать фотографии сетчатки и ошибка на порядок больше. Можно разделить фотографии по значению ошибки восстановления.\n",
        "\n",
        "В качестве \"испорченного датасета\" возьмем `val` чаcть из RetinaMnist и `test` часть из BloodMnist.\n",
        "\n",
        "Постройте гистограммы ошибок `mse_retina` и `mse_blood`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VWVZOvP2AT1"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDtW3XAU2AT1"
      },
      "source": [
        "Выберите порог ошибки восстановления и вычислите метрики качества отделения снимков сетчатки от снимков крови: accuracy, precision, recall, f1-score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X74CONam2AT1"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn7Hypz-2AT1"
      },
      "source": [
        "## Формат результата"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQ_L6UaT2AT2"
      },
      "source": [
        "Метрики качества отделения снимков сетчатки от снимков крови: accuracy, precision, recall, f1-score — не менее 0.95."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUjqiQIm2AT2"
      },
      "source": [
        "# Задание 3. Поиск hard examples с помощью автоэнкодера"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CUEmHU72AT2"
      },
      "source": [
        "В этом задании требуется найти самые \"некрасивые\" цифры в **обучающей выборке** MNIST. Найдите изображения цифр, которые хуже всего восстанавливаются автоэнкодером.\n",
        "\n",
        "Один из возможных подходов:\n",
        "* Обучить автоэнкодер на датасете MNIST.\n",
        "* Посчитать ошибку восстановления для каждой картинки обучающей выборки.\n",
        "* Отсортировать список ошибок восстановления и получить индексы изображений с наибольшей ошибкой.\n",
        "* Отрисовать такие изображения.\n",
        "\n",
        "В качестве критерия качества восстановления можно использовать как функцию потерь, так и другие меры сходства изображений, например, [structural similarity index 🛠️[doc]](https://scikit-image.org/docs/stable/auto_examples/transform/plot_ssim.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtMslGlF2AT2"
      },
      "source": [
        "## Формат результата"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNlrG6Lf2AT2"
      },
      "source": [
        "Вывести изображения обучающей выборки с наихудшим качеством восстановления автоэнкодером.\n",
        "\n",
        "Пример результата:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGNHctoN2AT2"
      },
      "source": [
        "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/Exercises/EX12/result_3_task_ex12.png\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jV8sHc-T2AT2"
      },
      "source": [
        "Импорт необходимых библиотек:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7tiLncI2AT3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import lightning as L\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchvision import transforms\n",
        "from collections import defaultdict\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from lightning.pytorch.loggers import TensorBoardLogger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXPeD4Fi2AT3"
      },
      "source": [
        "Загрузка датасета:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GrfazYm2AT3"
      },
      "outputs": [],
      "source": [
        "root = \"./data\"\n",
        "\n",
        "train_set = MNIST(root=root, train=True, transform=transforms.ToTensor(), download=True)\n",
        "torch.manual_seed(42)\n",
        "train_set, val_set = random_split(train_set, lengths=[50000, 10000])\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=64, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_set, batch_size=64, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXcEd5mk2AT3"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPHUEzSu2AT3"
      },
      "source": [
        "# Задание 4. Перенос стиля при помощи CVAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctFqH9Ly2AT3"
      },
      "source": [
        "С помощью CVAE можно решать задачу переноса стиля. Посмотрите на результат переноса стиля нескольких разных семерок на другие цифры:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4GiAIcx2AT3"
      },
      "source": [
        "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/Exercises/EX12/style_transfer.png\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9H7SWqK2AT4"
      },
      "source": [
        "Ваша задача:\n",
        "- Обучите CVAE, как в лекции (`latent_dim=2...10`). Вам понадобятся условный декодер `CDecoder` и `LitCVAE`.\n",
        "\n",
        "- Возьмите 10 случайных троек из тестового датасета и реализуйте перенос стиля троек на другие цифры.\n",
        "\n",
        "- Выведите визуализацию, как в примере. На вашей визуализации тройки должны быть исходные, а остальные цифры — полученные в результате переноса стиля."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJuA6sN82AT4"
      },
      "source": [
        "Импорт необходимых библиотек:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7dvSKQ52AT4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import lightning as L\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from lightning.pytorch.loggers import TensorBoardLogger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAW2vL3X2AT4"
      },
      "source": [
        "Загрузка датасета:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2BWCfKS2AT4"
      },
      "outputs": [],
      "source": [
        "root = \"./data\"\n",
        "\n",
        "train_set = MNIST(root=root, train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_set = MNIST(root=root, train=False, transform=transforms.ToTensor(), download=True)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "train_set, val_set = random_split(train_set, lengths=[50000, 10000])\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "val_loader = DataLoader(val_set, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "test_loader = DataLoader(test_set, batch_size=64, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sPzKfxo2AT4"
      },
      "outputs": [],
      "source": [
        "class VAEEncoder(Encoder):\n",
        "    def __init__(self, latent_dim):\n",
        "        if latent_dim % 2 != 0:  # check for the parity of the latent space\n",
        "            raise Exception(\"Latent size for VAEEncoder must be even\")\n",
        "\n",
        "        super().__init__(latent_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FecbxMsI2AT5"
      },
      "outputs": [],
      "source": [
        "class LitVAE(LitAE):\n",
        "    def __init__(self, encoder, decoder, kld_weight=0.005, recon_weight=1.0):\n",
        "        super().__init__(encoder, decoder)\n",
        "        self.kld_weight = kld_weight\n",
        "        self.recon_weight = recon_weight\n",
        "\n",
        "    def vae_split(self, latent):\n",
        "        size = (\n",
        "            latent.shape[1] // 2\n",
        "        )  # divide the latent representation into mu and log_var\n",
        "        mu = latent[:, :size]\n",
        "        log_var = latent[:, size:]\n",
        "        return mu, log_var\n",
        "\n",
        "    def vae_reparametrize(self, mu, log_var):\n",
        "        sigma = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn(mu.shape[0], mu.shape[1]).to(self.device)\n",
        "        return eps * sigma + mu\n",
        "\n",
        "    def kld_loss(self, mu, log_var):\n",
        "        var = log_var.exp()\n",
        "        kl_loss = torch.mean(-0.5 * torch.sum(log_var - var - mu**2 + 1, dim=1), dim=0)\n",
        "        return kl_loss\n",
        "\n",
        "    def forward_handler(self, data, *args, **kwargs):\n",
        "        # here is the logic how data is moved through AE\n",
        "        latent = self.encoder(data)\n",
        "\n",
        "        mu, log_var = self.vae_split(latent)\n",
        "        sample = self.vae_reparametrize(mu, log_var)\n",
        "\n",
        "        recon = self.decoder(sample)\n",
        "        return latent, recon\n",
        "\n",
        "    def loss_handler(self, recon, data, latent, *args, **kwargs):\n",
        "        mu, log_var = self.vae_split(latent)\n",
        "        # here is the loss function computing\n",
        "        loss = self.recon_weight * F.mse_loss(\n",
        "            F.sigmoid(recon), data\n",
        "        ) + self.kld_weight * self.kld_loss(mu, log_var)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp7NUEmK2AT5"
      },
      "source": [
        "1. Воспользуйтесь кодом для `CDecoder` и `LitCVAE` из лекции, чтобы обучить свой CVAE. В качестве энкодера используйте `VAEEncoder`. Достаточно обучать 5 эпох."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4swEWEY2AT5"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsGXfSRb2AT5"
      },
      "source": [
        "2. Обработайте тестовые данные с помощью CVAE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PM_XFbRd2AT6"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nm-vRiPX2AT6"
      },
      "source": [
        "3. Из словаря, полученного в результате обработки тестовых данных, отберите 10 случайных троек (вам помогут ключи `\"labels\"` и `\"real\"`). Выведите их с помощью функции `plot_samples`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOqrNPiK2AT6"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-dfUYrX2AT6"
      },
      "source": [
        "4. Теперь вам нужно обработать ваши тройки **энкодером** и получить для них латентные представления, кодирующие стиль начертания. Далее в цикле по $i$ от 0 до 9 подать в декодер латентные представления ваших троек и метку $i$-той цифры.\n",
        "\n",
        "Подсказка: вне цикла создайте пустой список `images`, а в цикле добавляйте в него результат работы декодера с $i$-той цифрой."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuuU_kBc2AT6"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4GicWo32AT7"
      },
      "outputs": [],
      "source": [
        "plot_samples(*images)  # result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OX-vmriu2AT7"
      },
      "source": [
        "## Формат результата"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjkGD-az2AT7"
      },
      "source": [
        "Перенести стили троек на другие цифры. При этом сами тройки должны вывестись исходные.\n",
        "\n",
        "Пример результата:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQiKqHJh2AT7"
      },
      "source": [
        "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/Exercises/EX12/result_4_task_ex12.png\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QW9sj1e22AT7"
      },
      "source": [
        "# Задание 5. Сиамская сеть"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHgDTXh82AT7"
      },
      "source": [
        "С помощью сиамской сети решим задачу с Kaggle.\n",
        "\n",
        "Цель: обучить сиамскую модель отличать поддельные подписи.\n",
        "\n",
        "[[doc] 🛠️ Signature verification dataset (Kaggle)](https://www.kaggle.com/robinreni/signature-verification-dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJB9FE3j2AT8"
      },
      "source": [
        "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/Exercises/EX12/signature_verification_dataset.png\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUGisXhv2AT8"
      },
      "source": [
        "Датасет состоит из набора сканов подписей, разложенных по папкам согласно следующей структуре:\n",
        "\n",
        "```\n",
        "sign_data_mini/\n",
        "├── train\n",
        "|   ├── 001\n",
        "|   |   ├── 001_01.PNG\n",
        "|   |   ├ ...\n",
        "|   |   └── 001_24.PNG\n",
        "|   ├── 001_forg\n",
        "|   |   └── ...\n",
        "|   ├ ...\n",
        "|   ├── 030\n",
        "|   |   └── ...\n",
        "|   └── 030_forg\n",
        "|       └── ...\n",
        "└── test\n",
        "    ├── 049\n",
        "    |   └── ...\n",
        "    |\n",
        "    ├── 049_forg\n",
        "    |   └── ...\n",
        "    ├ ...\n",
        "    ├── 051\n",
        "    |   └── ...\n",
        "    └── 051_forg\n",
        "        └── ...\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "В папке `id1` содержатся сканы подлинных подписей одного человека. В папке `id1_forg` содержатся сканы поддельных подписей того же человека.\n",
        "\n",
        "В папках с префиксом `id2`, `id2_forg` — настоящие и поддельные подписи другого человека и т. д.\n",
        "\n",
        "В папке `train` собраны оригинальные и поддельные подписи тридцати человек (`id = 001 ... 030`), а в папке `test` — оригинальные и поддельные подписи трех человек, отличных от тридцати \"обучающих\" (`id = 049 ... 051`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_4CaZaM2AT8"
      },
      "source": [
        "Установка и импорт необходимых библиотек:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvJgXiJW2AT8"
      },
      "outputs": [],
      "source": [
        "!pip install -q timm\n",
        "!pip install -q lightning\n",
        "!pip install -q torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7RLtEC22AT8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import timm\n",
        "import torch\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchmetrics\n",
        "import pandas as pd\n",
        "import lightning as L\n",
        "import seaborn as sns\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import nn\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "from itertools import product\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.nn.functional import cosine_similarity\n",
        "from lightning.pytorch.callbacks import ModelCheckpoint\n",
        "from lightning.pytorch.loggers import TensorBoardLogger\n",
        "from warnings import simplefilter\n",
        "\n",
        "simplefilter(\"ignore\", UserWarning)\n",
        "simplefilter(\"ignore\", RuntimeWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Загрузка датасета"
      ],
      "metadata": {
        "id": "EvXfQIw3Agim"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEasrLir2AT8"
      },
      "source": [
        "Загрузим фрагмент датасета, достаточный для выполнения задания:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0b8g6nk2AT8"
      },
      "outputs": [],
      "source": [
        "!wget -qN https://edunet.kea.su/repo/EduNet-web_dependencies/datasets/sign_mini.zip\n",
        "!unzip -qn sign_mini.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNRnIPqj2AT9"
      },
      "source": [
        "Класс датасета дан готовым.\n",
        "\n",
        "В отличие от датасетов для классификации, метод `__getitem__` возвращает кортеж не из двух, а из трех элементов:\n",
        "\n",
        "1. Реальная подпись, соответствующая индексу `index`.\n",
        "2. Подпись, с которой ее нужно сравнить. С вероятностью 0.5 это может быть другая настоящая подпись и с вероятностью 0.5 — подделка.\n",
        "3. Метка, соответствующая результату сопоставления. Для двух настоящих подписей это $1$. Для ситуации, когда одна из подписей поддельная, это $-1$.\n",
        "\n",
        "Необходимости изменять этот класс нет, однако это допустимо."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPqVctQt2AT9"
      },
      "outputs": [],
      "source": [
        "class SiameseNetworkDataset(Dataset):\n",
        "    def __init__(self, dir=None, transform=None):\n",
        "        self.dir = dir\n",
        "        self.transform = transform\n",
        "        self.classes = {1: \"Original\", -1: \"Forged\"}  # Change if need\n",
        "        self.data = self.get_pairs()\n",
        "        self.targets = self.get_targets()\n",
        "        self.cache = {}\n",
        "\n",
        "    def get_pairs(self):\n",
        "        pairs = []  # to store [orig, fake] or [orig, orig] pairs\n",
        "        persons = self.load_data()\n",
        "        for key in persons:\n",
        "            all_pairs = product(\n",
        "                persons[key][\"orig\"], persons[key][\"orig\"] + persons[key][\"forg\"]\n",
        "            )\n",
        "            # remove pairs with themselve\n",
        "            without_self_comparsion = list(filter(lambda x: x[0] != x[1], all_pairs))\n",
        "            pairs += without_self_comparsion\n",
        "        return pairs\n",
        "\n",
        "    def load_data(self):\n",
        "        all_paths = glob(f\"{self.dir}/**/*\")  # get all files path\n",
        "        persons = {}\n",
        "        # Group files by ID and type\n",
        "        for path in all_paths:\n",
        "            id, tp = SiameseNetworkDataset.parse(path)\n",
        "            if not id in persons:\n",
        "                persons[id] = {\"orig\": [], \"forg\": []}\n",
        "            persons[id][tp].append(path)\n",
        "        return persons\n",
        "\n",
        "    def get_targets(self):\n",
        "        targets = []\n",
        "        for pair in self.data:\n",
        "            _, tp = SiameseNetworkDataset.parse(pair[1])\n",
        "            label = -1 if tp == \"forg\" else 1\n",
        "            targets.append(label)\n",
        "        return targets\n",
        "\n",
        "    @staticmethod\n",
        "    def parse(path):\n",
        "        folder = path.split(os.sep)[-2]\n",
        "        id = folder.split(\"_\")[0]\n",
        "        tp = \"forg\" if \"forg\" in path else \"orig\"\n",
        "        return id, tp\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image1_path, image2_path = self.data[index]\n",
        "        labels = self.targets[index]\n",
        "\n",
        "        # Loading the images\n",
        "        image1 = self.load(image1_path)\n",
        "        image2 = self.load(image2_path)\n",
        "\n",
        "        return image1, image2, labels\n",
        "\n",
        "    def load(self, path):\n",
        "        if path in self.cache:\n",
        "            img = self.cache[path]\n",
        "        else:\n",
        "            img = Image.open(path).convert(\"L\")\n",
        "            self.cache[path] = img\n",
        "        # Apply image transformations\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        return img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2cje9Ro2AT9"
      },
      "source": [
        "Создадим экземпляр датасета и убедимся, что данные загружаются:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5gkO4zn2AT9"
      },
      "outputs": [],
      "source": [
        "# Viewing the sample of images to check whether its loading properly\n",
        "print('\"1\" - real, \"-1\" - fake')\n",
        "\n",
        "vis_dataset = SiameseNetworkDataset(\n",
        "    \"/content/sign_data_mini/train\",\n",
        "    transform=transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize((105, 105)),\n",
        "            transforms.ToTensor(),\n",
        "        ]\n",
        "    ),\n",
        ")\n",
        "\n",
        "vis_dataloader = DataLoader(vis_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "example_batch = next(iter(vis_dataloader))  # images1, images2, labels\n",
        "images1, images2, labels = example_batch\n",
        "# display the data\n",
        "concatenated = torch.cat((images1, images2), dim=0)\n",
        "grid = torchvision.utils.make_grid(concatenated)\n",
        "\n",
        "plt.figure(figsize=(20, 40))\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.imshow(grid.permute(1, 2, 0).numpy())\n",
        "plt.show()\n",
        "\n",
        "print(labels.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbexi1dP2AT9"
      },
      "source": [
        "Оценим баланс классов:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2lG3C2m2AT9"
      },
      "outputs": [],
      "source": [
        "# For check classes balance\n",
        "plt.hist(vis_dataset.targets)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Создание тренировочного и тестового датасетов и загрузчиков данных"
      ],
      "metadata": {
        "id": "5HThF3zGCGX2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02m8_oXw2AT-"
      },
      "source": [
        "В отличие от загрузчиков данных для визуализации, здесь в трансформациях присутствует нормализация.\n",
        "\n",
        "Этот блок можно модифицировать."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5F5yFZj2AT-"
      },
      "outputs": [],
      "source": [
        "# Define transforms include Normalization\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((105, 105)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(0.9409, 0.1078),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Load the dataset as pytorch tensors using dataloader\n",
        "sign_train_dataset = SiameseNetworkDataset(\n",
        "    dir=\"/content/sign_data_mini/train\", transform=transform\n",
        ")\n",
        "sign_train_loader = DataLoader(\n",
        "    sign_train_dataset, batch_size=128, shuffle=True, num_workers=2\n",
        ")\n",
        "\n",
        "# Load the test dataset\n",
        "sign_test_dataset = SiameseNetworkDataset(\n",
        "    dir=\"/content/sign_data_mini/test\", transform=transform\n",
        ")\n",
        "sign_test_loader = DataLoader(\n",
        "    sign_test_dataset, batch_size=8, shuffle=False, num_workers=2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCvukboU2AT-"
      },
      "source": [
        "## Создание сети"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6DppkXN2AT-"
      },
      "source": [
        "В данном блоке нужно описать структуру модели.\n",
        "\n",
        "На вход модели поступают **два** одноканальных изображения, а не одно (метод `forward` получает `input1` и `input2`).\n",
        "\n",
        "На выходе **два** вектора признаков (embeddings) `out1` и `out2`.\n",
        "\n",
        "Размер embedding выберите произвольно, достаточно 32–64 компонент.\n",
        "\n",
        "Допускается использовать в качестве основы готовые модели из torchvision или timm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5AMm_t12AT-"
      },
      "outputs": [],
      "source": [
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self, embedding_dim=32):\n",
        "        super().__init__()\n",
        "        # Your code here\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        # Your code here\n",
        "        return out1, out2\n",
        "\n",
        "\n",
        "model = SiameseNetwork()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek1OFlLc2AT-"
      },
      "source": [
        "## Метрика"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCE6Hd3F2AT-"
      },
      "source": [
        "Вспомогательный метод `emb2pred` служит для расчета косинусной схожести между двумя эмбеддингами и отнесения данной пары к классу $0$ или $1$ в зависимости от порога `threshold`.\n",
        "\n",
        "Две подписи считаются принадлежащими одному человеку, если косинусное сходство между их эмбеддингами больше порога. Иначе одна из подписей считается подделкой."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AH2VCrnO2AT_"
      },
      "outputs": [],
      "source": [
        "from torch.nn.functional import cosine_similarity\n",
        "\n",
        "\n",
        "def emb2pred(emb1, emb2, threshold):\n",
        "    sim = cosine_similarity(emb1, emb2)\n",
        "    pred = torch.empty_like(sim)\n",
        "    pred[sim < threshold] = 0  # Forged, not \"-1\" as in dataset for metric compute\n",
        "    pred[sim >= threshold] = 1  # Original\n",
        "    return pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHjuZmYB2AT_"
      },
      "source": [
        "Теперь можно считать метрику качества. В качестве метрики воспользуемся F1-score из torchmetrics. Убедимся, что на необученной модели она будет низкой.\n",
        "\n",
        "Для метрик бинарной классификации в torchmetrics ожидается, что метки классов принадлежат $\\{0, 1\\}$. Поэтому функция `emb2pred` выдает $0$ или $1$ в зависимости от порога, а метки $-1$ из датасета заменяются на $0$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XY8dvv12AT_"
      },
      "outputs": [],
      "source": [
        "metric = torchmetrics.classification.BinaryF1Score()\n",
        "\n",
        "model.eval()\n",
        "\n",
        "for batch in sign_test_loader:\n",
        "    images1, images2, labels = batch\n",
        "    emb1, emb2 = model(images1, images2)\n",
        "\n",
        "    preds = emb2pred(emb1, emb2, threshold=0.5)\n",
        "    labels[labels == -1] = 0  # for metric computation\n",
        "    metric.update(preds, labels)\n",
        "\n",
        "print(metric.compute().item())\n",
        "metric.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nl2lNFZY2AT_"
      },
      "source": [
        "## Обучение"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpKe3RIT2AT_"
      },
      "source": [
        "Блок кода для обучения. В качестве функции потерь предлагается использовать `torch.nn.CosineEmbeddingLoss` [🛠️[doc]](https://pytorch.org/docs/stable/generated/torch.nn.CosineEmbeddingLoss.html).\n",
        "\n",
        "Обратите внимание на то, что эта функция потерь [принимает при расчете 🛠️[doc]](https://pytorch.org/docs/stable/generated/torch.nn.CosineEmbeddingLoss.html#:~:text=Default%3A%20%27mean%27-,Shape%3A,-Input1%3A):\n",
        "* батч эмбеддингов №1,\n",
        "* батч эмбеддингов №2,\n",
        "* батч меток классов, состоящий из $1$ и $-1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZ9g1eLv2AT_"
      },
      "outputs": [],
      "source": [
        "class LitSiamese(L.LightningModule):\n",
        "    def __init__(self, model, threshold=0.5):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.threshold = threshold\n",
        "\n",
        "        self.criterion =  # Your code here\n",
        "\n",
        "        self.train_metric = torchmetrics.classification.BinaryF1Score()\n",
        "        self.val_metric = torchmetrics.classification.BinaryF1Score()\n",
        "        self.test_metric = torchmetrics.classification.BinaryF1Score()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.AdamW(self.parameters())\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # 1. unpack batch\n",
        "        # Your code here\n",
        "\n",
        "\n",
        "        # 2. do forward through model and loss, logging loss\n",
        "        # Your code here\n",
        "\n",
        "\n",
        "        # 3. get {0, 1} predictions with self.threshold,\n",
        "        #    change \"-1\" labels to \"0\" and update train_metric\n",
        "        # Your code here\n",
        "\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        # compute metric and do metric logging, reset train_metric\n",
        "        # Your code here\n",
        "\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        # all the same steps as in training_step\n",
        "        # but without `return loss`\n",
        "        # Your code here\n",
        "\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        # all the same as in on_train_epoch_end\n",
        "\n",
        "\n",
        "    def on_test_epoch_start(self):\n",
        "        # initialise test results dict, nothing to change here\n",
        "        self.test_similarity_dict = {\"similarity\": [], \"true_label\": []}\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        # 1. unpack batch\n",
        "        # Your code here\n",
        "\n",
        "\n",
        "        # 2. do forward through model\n",
        "        # Your code here\n",
        "\n",
        "\n",
        "        # 3. accumulating test results dict in order to get similarity distributions\n",
        "        #    (nothing to change here)\n",
        "        cosine_sim = F.cosine_similarity(emb1, emb2)\n",
        "        for i in range(labels.shape[0]):\n",
        "            self.test_similarity_dict[\"similarity\"].append(cosine_sim[i].item())\n",
        "            self.test_similarity_dict[\"true_label\"].append(labels[i].item())\n",
        "\n",
        "        # 4. get {0, 1} predictions with self.threshold,\n",
        "        #    change \"-1\" labels to \"0\" and update test_metric\n",
        "        # Your code here\n",
        "\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "        # compute and print test_metric value, reset test_metric\n",
        "        # Your code here\n",
        "\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        # implement forward in lightning model to use it directly, nothing to change here\n",
        "        return self.model(input1, input2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYPbmtIK2AUA"
      },
      "source": [
        "Для сохранения модели на промежуточных эпохах и для возможности последующего возврата к лучшей модели воспользуемся `ModelCheckpoint`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_uy9FF52AUA"
      },
      "outputs": [],
      "source": [
        "from lightning.pytorch.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    save_last=False,\n",
        "    every_n_epochs=1,\n",
        "    save_top_k=-1,\n",
        "    save_weights_only=True,\n",
        "    monitor=\"val_f1\",\n",
        "    filename=\"model-{epoch}-{val_f1:.4f}\",\n",
        "    mode=\"max\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYw-C7ja2AUA"
      },
      "source": [
        "Создание модели и запуск обучения. Достаточно 3 эпох."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jzo_Y03d2AUA"
      },
      "outputs": [],
      "source": [
        "from lightning.pytorch.loggers import TensorBoardLogger\n",
        "\n",
        "L.seed_everything(42)\n",
        "\n",
        "embedding_dim = 32\n",
        "pl_model = LitSiamese(SiameseNetwork(embedding_dim))\n",
        "\n",
        "logger = TensorBoardLogger(save_dir=\"lightning_logs\", name=\"task5_SiameseNetwork\")\n",
        "trainer = L.Trainer(\n",
        "    max_epochs=3, logger=logger, log_every_n_steps=1, callbacks=[checkpoint_callback]\n",
        ")\n",
        "\n",
        "trainer.fit(pl_model, sign_train_loader, sign_test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ktnao062AUA"
      },
      "source": [
        "## Тестирование"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDWRQj5c2AUA"
      },
      "source": [
        "Вспомогательный код для вывода изображений:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgvLKsya2AUA"
      },
      "outputs": [],
      "source": [
        "def imshow(img, text=None):\n",
        "    npimg = (\n",
        "        img * 0.1078 + 0.9409\n",
        "    ).numpy()  # canceling transforms.Normalize(0.9409, 0.1078)\n",
        "    plt.figure(figsize=(5, 3))\n",
        "    plt.axis(\"off\")\n",
        "    plt.rcParams[\"figure.figsize\"] = (10, 20)\n",
        "    if text:\n",
        "        plt.text(\n",
        "            110,\n",
        "            8,\n",
        "            text,\n",
        "            style=\"italic\",\n",
        "            fontweight=\"bold\",\n",
        "            bbox={\"facecolor\": \"white\", \"alpha\": 0.8, \"pad\": 10},\n",
        "        )\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1n1vMAU2AUB"
      },
      "source": [
        "Визуализация результатов сравнения на одном батче тестовой выборки:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kwrB43z2AUB"
      },
      "outputs": [],
      "source": [
        "# Print the sample outputs to view its ssimilarity\n",
        "batch = next(iter(sign_test_loader))\n",
        "\n",
        "images1, images2, labels = batch\n",
        "emb1, emb2 = pl_model(images1, images2)\n",
        "\n",
        "cosine_sim = F.cosine_similarity(emb1, emb2)\n",
        "\n",
        "for i in range(labels.shape[0]):\n",
        "    concatenated = torch.cat((images1[i][0], images2[i][0]), 1)\n",
        "    label = sign_test_dataset.classes[labels[i].item()]\n",
        "    imshow(\n",
        "        torchvision.utils.make_grid(concatenated),\n",
        "        \"Cosine similarity: {:.2f} True label: {}\".format(\n",
        "            cosine_sim[i].item(),\n",
        "            label,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    print(f\"Cosine similarity  {cosine_sim[i].item():.2f} (range [-1 .. 1])\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXa45FXk2AUB"
      },
      "source": [
        "Посмотрим на распределение схожести (Cosine Similarity) на изображениях тестовой выборки в разрезе по истинным классам.\n",
        "\n",
        "Для этого запустим тестирование модели. После тестирования у модели появится атрибут `.test_similarity_dict`, который будет содержать величину косинусной схожести для каждой из пар изображений в тестовой выборке, а также метку класса для данной пары."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTr-gjjI2AUB"
      },
      "outputs": [],
      "source": [
        "trainer.test(dataloaders=sign_test_loader, ckpt_path=\"best\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc4hRNLI2AUC"
      },
      "source": [
        "Отобразим распределения схожестей для тестовых пар в разрезе по метке класса."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8h7D4nF2AUC"
      },
      "outputs": [],
      "source": [
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "test_results = pl_model.test_similarity_dict\n",
        "\n",
        "sim_df = pd.DataFrame(test_results)\n",
        "sim_df[\"label\"] = sim_df[\"true_label\"].map(sign_test_dataset.classes)\n",
        "display(sim_df)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.histplot(sim_df, x=\"similarity\", hue=\"label\", bins=20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8Dbkgkj2AUC"
      },
      "source": [
        "Какой порог косинусной схожести лучше выбрать, чтобы отличать оригинальные и поддельные подписи?\n",
        "\n",
        "***Your text here***\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-byX8p_2AUC"
      },
      "source": [
        "Финальная оценка точности с выбранным порогом:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8e8pBee2AUC"
      },
      "outputs": [],
      "source": [
        "pl_model.threshold =  # Your value here\n",
        "trainer.test(dataloaders=sign_test_loader, ckpt_path=\"best\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKsC9ky22AUD"
      },
      "source": [
        "## Формат результата\n",
        "\n",
        "* Реализованный код обучения сиамской сети\n",
        "* Результаты обучения\n",
        "* Оценка метрики качества на тестовой выборки больше 0.8"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}